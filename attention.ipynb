{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from glob import iglob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOCAB파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13841, list, 13841, dict)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_file = './vocab.csv'\n",
    "with open(vocab_file, 'r') as f:\n",
    "    words = csv.reader(f)\n",
    "    vocabs = []\n",
    "    for i, word in enumerate(words):\n",
    "        vocabs.append(word[0])\n",
    "word_to_id = {w:i for i,w in enumerate(vocabs)}\n",
    "len(vocabs), type(vocabs), len(word_to_id), type(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = 'data_samples'\n",
    "subdirectories = [os.path.join(data_samples,o) for o in os.listdir(data_samples) if os.path.isdir(os.path.join(data_samples,o))]\n",
    "python_files = [(directory, [y for x in os.walk(directory) for y in iglob(os.path.join(x[0], '*.py'))])\n",
    "                for directory in subdirectories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = python_files[0][0]\n",
    "file_names = python_files[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapping = (lambda x: x) if word_to_id is None else (lambda x: word_to_id.get(x, oov_id))\n",
    "def get_source_tree(filename):\n",
    "    print(filename)\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        fstr = f.read()\n",
    "    fstr = fstr.replace('\\\\n', '\\n').replace('\\r', '\\n')\n",
    "    if not fstr.endswith('\\n'):\n",
    "        fstr += '\\n'\n",
    "    return fstr, ast.parse(fstr, filename=filename)\n",
    "\n",
    "def preprocess(tokentype, tokenval):\n",
    "    if tokentype == tokenize.NUMBER:\n",
    "        return number_token\n",
    "\n",
    "    elif tokentype == tokenize.INDENT:\n",
    "        return indent_token\n",
    "\n",
    "    elif tokentype == tokenize.DEDENT:\n",
    "        return dedent_token\n",
    "\n",
    "    return tokenval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_samples\\aio-libs\\aiohttp\\setup.py\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (setup.py, line 59)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"D:\\python3_5anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2862\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[0;32m\"<ipython-input-83-24db8c3aa804>\"\u001b[0m, line \u001b[0;32m9\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    source, tree = get_source_tree(filename)\n",
      "  File \u001b[0;32m\"<ipython-input-73-9815768462f0>\"\u001b[0m, line \u001b[0;32m9\u001b[0m, in \u001b[0;35mget_source_tree\u001b[0m\n    return fstr, ast.parse(fstr, filename=filename)\n",
      "\u001b[1;36m  File \u001b[1;32m\"D:\\python3_5anaconda\\lib\\ast.py\"\u001b[1;36m, line \u001b[1;32m35\u001b[1;36m, in \u001b[1;35mparse\u001b[1;36m\u001b[0m\n\u001b[1;33m    return compile(source, filename, mode, PyCF_ONLY_AST)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"data_samples\\aio-libs\\aiohttp\\setup.py\"\u001b[1;36m, line \u001b[1;32m59\u001b[0m\n\u001b[1;33m    var2981 = dict(name='aiohttp', version=var4375, description='Async http client/server framework (asyncio)', long_description='\\n\\n'.join((function1340('README.rst'), function1340('CHANGES.rst'))), classifiers=['License :: OSI Approved :: Apache Software License', 'Intended Audience :: Developers', 'Programming Language :: Python', 'Programming Language :: Python :: 3', 'Programming Language :: Python :: 3.4', 'Programming Language :: Python :: 3.5', 'Programming Language :: Python :: 3.6', 'Development Status :: 5 - Production/Stable', 'Operating System :: POSIX', 'Operating System :: MacOS :: MacOS X', 'Operating System :: Microsoft :: Windows', 'Topic :: Internet :: WWW/HTTP', 'Framework :: AsyncIO'], author='Nikolay Kim', author_email='fafhrd91@gmail.com', maintainer=', '.join(('Nikolay Kim <fafhrd91@gmail.com>', 'Andrew Svetlov <andrew.svetlov@gmail.com>')), maintainer_email='aio-libs@googlegroups.com', url='https://github.com/aio-libs/aiohttp/', license='Apache 2', packages=['aiohttp'], install_requires=var777, tests_require=var536, include_package_data=True, ext_modules=var391, cmdclass=dict(build_ext=Class4, test=Class249))\u001b[0m\n\u001b[1;37m                                                                                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "definition_positions = []\n",
    "identifier_usage = []\n",
    "gen_def_positions = True\n",
    "error_data = 0\n",
    "data_file_name = []\n",
    "for filename in file_names:\n",
    "#     try:\n",
    "        source, tree = get_source_tree(filename)\n",
    "        tokens = tokenize.generate_tokens(StringIO(source).readline)\n",
    "        data.append([(mapping(preprocess(tokenType, tokenVal)), start) for tokenType, tokenVal, start, _, _\n",
    "                     in tokens\n",
    "                     if tokenType != tokenize.COMMENT and\n",
    "                     not tokenVal.startswith(\"'''\") and\n",
    "                     not tokenVal.startswith('\"\"\"') and \n",
    "                     (tokenType == tokenize.DEDENT or tokenVal != \"\")])\n",
    "        data_file_name.append(filename)\n",
    "        if gen_def_positions:\n",
    "            walker = astwalker.ASTWalker()\n",
    "            walker.walk(tree)\n",
    "            definition_positions.append(walker.definition_positions)\n",
    "            identifier_usage.append(walker.name_usage)\n",
    "#     except:\n",
    "#         error_data+=1\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
